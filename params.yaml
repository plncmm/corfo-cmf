filepath: ../data/input_files/seguros_valores_tipo_materia.xlsx # Directorio del archivo asociado de la clasificación.
sample_frac: 1 # Decimal que representa el porcentaje de datos a muestrear del dataset leído. Para debuggear es recomendable usar un subconjunto pequeño.
model: beto # Modelo a utilizar, las opciones son: beto, roberta, lstm, cnn, logistic, nv, random_forest, svm, xgboost.
device: cuda # Utilizar cuda o cpu
cuda_id: 0 # ID asociado a la gpu en caso que se seleccione la opción cuda en el parámetro anterior.

pre_processing:
  label: TIPO_MATERIA # Seleccionar que clasificación se realiza, las opciones son: MERCADO_INGRESO, TIPO_ENTIDAD, TIPO_PRODUCTO, TIPO_MATERIA, NOMBRE_ENTIDAD
  do_lower_case: True # Pasar las palabras a minúsculas antes del entrenamiento. Esto es recomendable                         
  remove_punctuation: False # Remover puntuaciones
  remove_stopwords: False # Remover las palabras más comunes del Español
  remove_frequent_words: False # Remover palabras frecuentes en el corpus
  stemming: False # Realizar un stemming 
  remove_short_examples: False # Remover ejemplos cortos
  lemmatization: False # Lematizar las palabras (ToDo)
  
beto_config: 
    train_size: 0.8 # Proporción de datos a utilizar en el conjunto de entrenamiento. [0,1]
    val_size: 0.1 # Proporción de datos a utilizar en el conjunto de validación. [0,1]
    test_size: 0.1 # Proporción de datos a utilizar en el conjunto de test. [0,1]
    version: uncased # Elegir si utilizar el modelo de BETO uncased (entrenado con texto en minúsculas) o la versión cased entrenado sobre todos los casos.
    batch_size: 16 # Tamaño del batch a utilizar en el entrenamiento.
    epochs: 3  # Número de épocas a entrenar el modelo.
    lr: 2e-5  # Tasa de aprendizaje.
    eps: 1e-8 # Adam epsilon
    weight_decay_rate: 0.01 # Regularización
    num_warmup_steps: 0 # Valor recomendado si no es 0: 10% * datasetSize/batchSize
    max_len: 512 # Máximo largo de secuencia
    output_dir: ../models/seguros_valores_tipo_materia # Directorio donde se guardará el modelo pre-entrenado.

roberta_config: 
    train_size: 0.8
    val_size: 0.1
    test_size: 0.1
    batch_size: 8 
    epochs: 3 
    lr: 2e-5 
    eps: 1e-8
    weight_decay_rate: 0.01
    num_warmup_steps: 0
    max_len: 512
    output_dir: ../models/roberta_models

cnn_config:
    train_size: 0.9
    test_size: 0.1
    smote: True
    pre_trained_embs: True
    trainable_embs: True
    embeddings_path: '../data/embeddings/SBW-vectors-300-min5.txt'
    emb_dim: 300
    output_dir: ../models/cnn_models
    batch_size: 1024
    epochs: 3
    dropout: 0.5
    dense_units: 128
    filters: 100
    kernel_size: 4

lstm_config:
    train_size: 0.9
    test_size: 0.1
    smote: True
    pre_trained_embs: True
    trainable_embs: True
    embeddings_path: '../data/embeddings/SBW-vectors-300-min5.txt'
    emb_dim: 300
    output_dir: ../models/lstm_models
    batch_size: 1024
    epochs: 3
    dropout: 0.2
    hidden_units: 128
   
random_forest_config:
    train_size: 0.9
    test_size: 0.1
    smote: True
    emb_dim: 300
    n_estimators: 150
    verbose: True
    analyzer: word
    max_features: 100000
    use_embeddings: True
    balanced: True
    embeddings_path: '../data/embeddings/SBW-vectors-300-min5.txt'
    output_dir: ../models/random_forest_models

logistic_config:
    train_size: 0.9
    test_size: 0.1
    smote: True
    emb_dim: 300
    verbose: 4
    max_features: 100000
    use_embeddings: False
    embeddings_path: '../data/embeddings/SBW-vectors-300-min5.txt'
    output_dir: ../models/logistic_models
    analyzer: word

svm_config:
    train_size: 0.9
    test_size: 0.1
    smote: True
    emb_dim: 300
    C: 1.0   
    kernel: linear
    degree: 3
    gamma: auto
    class_weight: balanced
    max_features: 100000
    use_embeddings: False
    embeddings_path: '../data/embeddings/SBW-vectors-300-min5.txt'
    output_dir: ../models/svm_models
    analyzer: word

nv_config:
    train_size: 0.9
    test_size: 0.1
    emb_dim: 300
    smote: True
    max_features: 100000
    use_embeddings: False # No se utilizarán: ValueError: Negative values in data passed to MultinomialNB (input X), me tira ese error
    embeddings_path: '../data/embeddings/SBW-vectors-300-min5.txt'
    output_dir: ../models/nv_models
    analyzer: word

xgboost_config:
    train_size: 0.9
    test_size: 0.1
    emb_dim: 300
    smote: True
    max_features: 100000
    use_embeddings: True
    embeddings_path: '../data/embeddings/SBW-vectors-300-min5.txt'
    output_dir: ../models/xgboost_models
    analyzer: word